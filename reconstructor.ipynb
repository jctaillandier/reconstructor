{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to generate Adult Data and pass it in self.data\n",
    "<ul>\n",
    "    <li> <del>environment virtuel python sur cedar\n",
    "    <li> <del>Make sure GPU is used\n",
    "    <li><del> Need ask best params from Rosin</li>\n",
    "    <li> <del>run Adult\n",
    "    <li> <del> make sure toSave data from PreProcessing somewhere\n",
    "    <li> <del>Wrap to take two csv of corresponding original (label)\n",
    "    <li> <del>Code test testing loop\n",
    "    <li> <del>Same Encoding for labels and training data\n",
    "    <li> Remove Sex attribute from training data\n",
    "    <li> <b>W\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Is dataloaders right shape\n",
    "<li>Writing tensor row by row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *** \n",
      " Currently running on cpu\n",
      " *** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Imports\n",
    "import utilities as utils\n",
    "# import datasets as d\n",
    "\n",
    "# from Stats import Plots as Pl\n",
    "import tqdm\n",
    "import time\n",
    "import csv\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import os.path\n",
    "import warnings\n",
    "import importlib\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from torch.utils.data import *\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data as td\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "CPU_DEVICE = torch.device(\"cpu\")\n",
    "GET_VALUE = lambda x: x.to(CPU_DEVICE).data.numpy().reshape(-1)[0]\n",
    "print(f\"\\n *** \\n Currently running on {device}\\n *** \\n\")\n",
    "with open('./data/full_original.csv', 'r') as r:\n",
    "        read = csv.reader(r)\n",
    "        header = next(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data import & Pre-processing\n",
    "  \n",
    "class My_dataLoader:\n",
    "    def __init__(self, batch_size : int, data_path :str, label_path:str, n_train :int,  label_col_name:str, test_batch_size:int=128):\n",
    "        '''\n",
    "            Creates train and test loaders from local files, to be easily used by torch.nn\n",
    "            \n",
    "            :batch_size: int for size of training batches\n",
    "            :data_path: path to csv where data is. 2d file\n",
    "            :label_path: csv containing labels. line by line equivalent to data_path file\n",
    "            :n_train: int for the size of training set (assigned randomly)\n",
    "            :test_batch: size of batches at test time. If none, will be same \n",
    "                    as training\n",
    "            :label_col_name name of columns that contains labels\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = n_train\n",
    "        \n",
    "        df_data = pd.read_csv(data_path)\n",
    "        df_label = pd.read_csv(label_path)\n",
    "        \n",
    "        a = df_data.values\n",
    "        b = df_label.values\n",
    "        self.trdata = torch.tensor(a[:self.train_size,:]) # where data is 2d [D_train_size x features]\n",
    "        self.trlabels = torch.tensor(b[:self.train_size,:]) # also has too be 2d\n",
    "        self.tedata = torch.tensor(a[self.train_size:,:]) # where data is 2d [D_train_size x features]\n",
    "        self.telabels = torch.tensor(b[self.train_size:,:]) # also has too be 2d\n",
    "        \n",
    "        self.train_dataset = torch.utils.data.TensorDataset(self.trdata, self.trlabels)\n",
    "        self.test_dataset = torch.utils.data.TensorDataset(self.tedata, self.telabels)\n",
    "\n",
    "        # Split dataset into train and Test sets\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=1,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        if test_batch_size == 'full':\n",
    "            test_batch_size = len(self.test_dataset)\n",
    "            \n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=test_batch_size,\n",
    "            num_workers=1,\n",
    "            pin_memory=False\n",
    "        )\n",
    "\n",
    "class PreProcessing:\n",
    "    def __init__(self, params_file: str):\n",
    "        '''\n",
    "            Imports all variables/parameters necessary for preparation of training.\n",
    "            Looks into params.yaml, then creates dataloader that can be used\n",
    "            \n",
    "            params_file: path to file that contains parameters.\n",
    "                            Needs to follow a specific naming and format\n",
    "        '''\n",
    "        # Import params\n",
    "        stream = open(params_file, 'r')\n",
    "        self.params = yaml.load(stream, yaml.FullLoader)\n",
    "\n",
    "        import_path = self.params['data_loading']['data_path']['value']\n",
    "        label_path = self.params['data_loading']['label_path']['value']\n",
    "\n",
    "        batchSize = self.params['model_params']['batchSize']['value']        \n",
    "        test_batch_size = self.params['model_params']['test_batch_size']['value']        \n",
    "        percent_train_set = self.params['model_params']['percent_train_set']['value']\n",
    "\n",
    "\n",
    "        df2 = pd.read_csv(import_path)\n",
    "        df_labels = pd.read_csv(label_path)\n",
    "        n_test = int(len(df2.iloc[:,0])*percent_train_set)\n",
    "\n",
    "        if df2.shape != df_labels.shape:\n",
    "            print(df2.shape)\n",
    "            print(df_labels.shape)\n",
    "            raise ValueError(\"The labels csv and data csv don't have the same shape.\")\n",
    "        \n",
    "        data_cat_list = utils.find_cat(df2)\n",
    "        label_cat_list = utils.find_cat(df_labels)\n",
    "        \n",
    "        if data_cat_list != label_cat_list:\n",
    "            raise ValueError('The categorical data columns found in your data doesnt match the ones found in your labels.\\n That means the AE will learn on data that is encoded differently from the labels it tries to immitate.') \n",
    "       \n",
    "        \n",
    "        if len(data_cat_list) > 0:\n",
    "            print(f\"Categorical variable found. Running Pandas dummy_variable encoding on {len(data_cat_list)} columns \\n\")\n",
    "            df_data = utils.dummy_encode(df2, data_cat_list)\n",
    "            df_labels = utils.dummy_encode(df_labels, label_cat_list)\n",
    "            print(f\"Saving output dataset under {import_path[:-4]}_NoCat.csv \\n\")\n",
    "            df_data.to_csv(f\"{import_path[:-4]}_NoCat.csv\", index=False)\n",
    "            \n",
    "            df_data, df_labels = utils.adjust_enc_errors(df_data, df_labels)\n",
    "            if df_data.shape != df_labels.shape:\n",
    "                print(df_data.shape)\n",
    "                print(df_labels.shape)\n",
    "                raise ValueError(\"The labels csv and data post-encoding don't have the same shape.\")\n",
    "                \n",
    "            df_labels.to_csv(f\"{label_path[:-4]}_NoCat.csv\", index=False)\n",
    "            self.dataloader = My_dataLoader(batchSize, f\"{import_path[:-4]}_NoCat.csv\", f\"{label_path[:-4]}_NoCat.csv\", n_test, \"income\", test_batch_size)\n",
    "        \n",
    "        else: # no categorical vars found\n",
    "            self.dataloader = My_dataLoader(batchSize, import_path, n_test, \"income\", test_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, 16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.Sigmoid(), nn.Linear(8, 4))#, nn.ReLU(True), nn.Linear(4, 4))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(4, 16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(0.25),\n",
    "#             nn.Linear(10, 16),\n",
    "#             nn.ReLU(True),\n",
    "            nn.Linear(16, 24),\n",
    "            nn.Sigmoid(), nn.Linear(24, out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "def train(model,train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
    "      \n",
    "        #inputs = inputs.view(batchSize, 1,100,100)\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs.float())\n",
    "        loss = loss_fn(output.float(), target.float())\n",
    "        train_loss.append(loss)\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_loss = sum(train_loss) / batch_idx+1\n",
    "    return mean_loss\n",
    "\n",
    "def test(model, test_loader, test_loss_fn, last_epoch=False):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_size = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in test_loader:\n",
    "\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            \n",
    "            if last_epoch == True:\n",
    "                data = inputs.tolist()\n",
    "                with open(f\"./experiments/{str.replace(time.ctime()[4:-8], ' ', '_')}-original_testset.csv\", 'w', newline=\"\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(header)\n",
    "                    writer.writerows(data)\n",
    "                \n",
    "            output = model(inputs.float())\n",
    "\n",
    "            if last_epoch == True:\n",
    "                data = output.tolist()\n",
    "                with open(f\"./experiments/{str.replace(time.ctime()[4:-8], ' ', '_')}-generated_testset.csv\", 'w', newline=\"\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(header)\n",
    "                    writer.writerows(data)\n",
    "            \n",
    "            test_size += len(inputs.float())\n",
    "            test_loss += test_loss_fn(output.float(), target.float()).item() \n",
    "    test_loss /= test_size\n",
    "\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=16, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=8, out_features=4, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Dropout(p=0.25, inplace=False)\n",
       "    (1): Linear(in_features=4, out_features=16, bias=True)\n",
       "    (2): Sigmoid()\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "    (4): Linear(in_features=16, out_features=24, bias=True)\n",
       "    (5): Sigmoid()\n",
       "    (6): Linear(in_features=24, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Autoencoder(20,20)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variable found. Running Pandas dummy_variable encoding on 7 columns \n",
      "\n",
      "Saving output dataset under ./data/full_a=0_E=19_NoCat.csv \n",
      "\n",
      "Epoch 0 running...\n",
      "Epoch 0 complete. Test Loss: 0.4900 \n",
      "\n",
      "Epoch 1 running...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4d6abcc2f813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreProcessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"params.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-4d6abcc2f813>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(experiment_x, model_type)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch} running...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mbatch_ave_tr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexperiment_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mave_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ave_tr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-0077ae5e7aa4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#inputs = inputs.view(batchSize, 1,100,100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(experiment_x: PreProcessing, model_type:str='autoencoder'):\n",
    "    '''\n",
    "        Takes care of all model training, testing and generation of data\n",
    "        \n",
    "        experiment_x: PreProcessing object that contains all data and parameters\n",
    "    \n",
    "    '''\n",
    "    start = time.time()\n",
    "    wd = experiment_x.params['model_params']['weight_decay']['value']\n",
    "    learning_rate = experiment_x.params['model_params']['learning_rate']['value']\n",
    "\n",
    "    num_epochs = int(experiment_x.params['model_params']['num_epochs']['value'])\n",
    "\n",
    "    in_dim = experiment_x.dataloader.trdata.shape[1]\n",
    "    out_dim = experiment_x.dataloader.trlabels.shape[1]\n",
    "    #######\n",
    "\n",
    "    if model_type == 'autoencoder':\n",
    "        model = Autoencoder(in_dim, out_dim).to(device)\n",
    "    train_loss = torch.nn.L1Loss().to(device) # BCEWithLogitsLoss combines sig, so no need activate out layer with softmax\n",
    "    test_loss_fn =torch.nn.L1Loss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=wd)\n",
    "    test_accuracy = []\n",
    "    ave_train_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch} running...\")\n",
    "\n",
    "        batch_ave_tr_loss = train(model,experiment_x.dataloader.train_loader, optimizer, train_loss)\n",
    "        ave_train_loss.append(batch_ave_tr_loss)\n",
    "\n",
    "        last = False\n",
    "        if epoch+1 == num_epochs: # then save the test batch input and output for metrics\n",
    "            last = True\n",
    "        loss = test(model, experiment_x.dataloader.test_loader, test_loss_fn, last)\n",
    "\n",
    "        print(f\"Epoch {epoch} complete. Test Loss: {loss:.4f} \\n\")      \n",
    "        test_accuracy.append(loss)\n",
    "\n",
    "\n",
    "    a = experiment_x.params['data_loading']['data_path']['value'][36:].split('/')[0]\n",
    "\n",
    "    fm = open(f\"./experiments/{str.replace(time.ctime(), ' ', '_')}-{a}.pth\", \"wb\")\n",
    "    torch.save(model.state_dict(), fm)\n",
    "\n",
    "    with open(f\"./experiments/{str.replace(time.ctime(), ' ', '_')}-{a}.txt\", 'w+') as f:\n",
    "        f.write(f\"Epochs: {num_epochs} \\n\")\n",
    "        f.write(f\"Learning Rate: {learning_rate} \\n\")\n",
    "        f.write(f\"weight decay: {wd}\\n\")\n",
    "        f.write(f\"Training Loss: {str(train_loss)}\\n\")\n",
    "        f.write(f\"Test Loss: {str(test_loss_fn)} \\n\")\n",
    "        f.write(f\"Optimizer: {str(optimizer)}\\n\")\n",
    "        f.write(f\"Train loss values: {str(ave_train_loss)} \\n\")\n",
    "        f.write(f\"Test loss values: {str(test_accuracy)}\\n\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Training on {num_epochs} epochs completed in {(end-start)/60} minutes.\")\n",
    "    \n",
    "experiment = PreProcessing(\"params.yaml\")\n",
    "\n",
    "train_model(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(1,num_epochs+1), np.array(test_accuracy))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"L1 Loss\")\n",
    "plt.title(\"Test Loss\")\n",
    "plt.savefig(f\"./experiments/{str.replace(time.ctime(), ' ', '_')}-{a}_test-loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-72b20c4aec5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"L1 Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Loss - average per epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./experiments/{str.replace(time.ctime(), ' ', '_')}-{a}_train-loss.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.plot(np.arange(1,num_epochs+1), np.array(train_losses))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"L1 Loss\")\n",
    "plt.title(\"Train Loss - average per epoch\")\n",
    "plt.savefig(f\"./experiments/{str.replace(time.ctime(), ' ', '_')}-{a}_train-loss.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
