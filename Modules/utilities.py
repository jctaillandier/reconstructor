from typing import List
from Modules import datasets as d
from Modules import analysisTools as at
import pandas as pd
import os
import torch
import argparse


    
def encode(data_path: str)-> d.Encoder:
    '''
        Encodes the same way gansan does. It returns an object that contains the meta data
        about the encoding and will be need to call decode() and retrieve original columns
        
        :PARAMS
        data_path: Path to the original data to be encoded, and where .prm is situated
        
        return: d.Preprocessing obj; obj.df returns data in dataframe, 
                                        obj.inverse_transform() to reverse encoding
    '''

    d_i = d.Encoder(data_path)
    d_i.load_parameters(prmPath='./data/')
    d_i.transform()
    
    return d_i
    

def check_dir_path(path_to_check: str) -> str:
    '''
        Checks if provided path is currently a at current level directory.
        If it is, it appends a number to the end and checks again 
        until no directory with such name exists

        :PARAMS
        path_to_check: str The path to location to check

        return: str New path with which os.mkdir can be called
    '''
    new_path = path_to_check
    if os.path.isdir(path_to_check):
        print("Experiment with name: \'{}\' already exists. Appending int to folder name. ".format(path_to_check))
        if os.path.isdir(path_to_check):
            expand = 1
            while True:
                expand += 1
                new_path = path_to_check[:-1] + '_' + str(expand) + '/'
                if os.path.isdir(new_path):
                    continue
                else:
                    break
            print(f"Experiment path: {new_path} \n \n ")
    return new_path

def parse_arguments(parser):
    parser.add_argument('--batch_size', type=int, default=128, help='The dimension size of the embedding, which will be generated by the generator. (default value: 128)')

    parser.add_argument('--generate_data', type=str2bool, default=False, help='If True the model generates data, if False the model is trained (default value: False)')

    args = parser.parse_args()
    return args

def tensor_to_df(tensor: torch.Tensor, headers: List[str]) -> pd.DataFrame:
    '''
        Takes in a 2d or 3d tensor and its column list and returns datafram
        if 3d, we assume first dim (dim=0) is the batch size, hence its ignored


    :PARAMS
    tensor: tensor to convert
    headers: list of headers to assign in that order. Must be same size 
                as last dim of tensor parameter.

    '''
    if tensor.shape[-1] != len(headers):
        raise ValueError(f"Tensor's last dimension ({tensor.shape[-1]}) must match headers length ({len(headers)})")

    
    return pd.DataFrame (tensor.tolist(), columns=headers)

def three_way_viz(df1: pd.core.frame.DataFrame, df2: pd.core.frame.DataFrame, df3: pd.core.frame.DataFrame, path_to_exp: str, red:str='pca'):
    '''
        I haven't paid enough attention in my data structure class, and I can't find a way 
            to make this 3 way iteration of all combination with dataframes without bugs
            I decided to hide this in utils, since I am ashamed
        Calculates Damage and Diversity and saves output in experimetn folder

        :PARAMS
        3X df_x: the three dataframe you want to compare
        path_to_exp: path to base of experiment, leading to ...experiment/<exp_name>/
        red: reduction chosen, can choose between pca, tsne, umap or svp    
    '''

    # Damage 
    dr = at.DimensionalityReduction()
    dr.clusters_original_vs_transformed_plots({"original": df1, "transformed": df2},
                                            labels=df1[20], dimRedFn=red,
                                            savefig=path_to_exp+f"dim_reduce/og_san_damage.png")
    dr.original_vs_transformed_plots({"original": df1, "transformed": df2}, dimRedFn=red,
                                        savefig=path_to_exp+f"dim_reduce/og_san_damage.png")

    # Then Diversity 
    div = at.Diversity()
    diversity = div(test_data_dict[key][0], f"original_og_san")
    diversity.update(div(test_data_dict[key][1], f"transformed_og_san"))


        # Damage 
    dr = at.DimensionalityReduction()
    dr.clusters_original_vs_transformed_plots({"original": df2, "transformed": df3},
                                            labels=df2[20], dimRedFn=red,
                                            savefig=path_to_exp+f"dim_reduce/san_gen_damage.png")
    dr.original_vs_transformed_plots({"original": df2, "transformed": df3}, dimRedFn=red,
                                        savefig=path_to_exp+f"dim_reduce/san_gen_damage.png")

    # Then Diversity 
    div = at.Diversity()
    diversity = div(df1, f"original_san")
    diversity.update(div(df2, f"transformed_gen"))



        # Damage 
    dr = at.DimensionalityReduction()
    dr.clusters_original_vs_transformed_plots({"original": df3, "transformed": df1},
                                            labels=df3[20], dimRedFn=red,
                                            savefig=path_to_exp+f"dim_reduce/gen_og_damage.png")
    dr.original_vs_transformed_plots({"original": df3, "transformed": df1}, dimRedFn=red,
                                        savefig=path_to_exp+f"dim_reduce/gen_of_damage.png")

    # Then Diversity 
    div = at.Diversity()
    diversity = div(test_data_dict[key][0], f"original_gen")
    diversity.update(div(test_data_dict[key][1], f"transformed_original"))