Epochs: 1500 
 
Total of 0 / 100 are now closer to original data (L1 distance). 
 
Epoch of lowest loss: 175 
 
Average lowest loss: 67016705.0809375175 
 
Lowest lost Generated:
 [2013759.125, 4520544256.0, 15549899.0, 14760639.0, 9705995.0, 111524216.0, 4731521.0, 5378094.5, 1992979.875, 851444.3125, 7449295.5, 12716660.0, 20432818.0, 27806864.0, 9628786.0, 19153836.0, 922074.3125, 3588790.0, 40746348.0, 6465342.0, 6034147.0, 3860398.5, 2165184.0, 36463928.0, 38634516.0, 171343.34375, 7731728.5, 6924988.5, 4157876.75, 41719544.0, 21267126.0, 27887880.0, 12940380.0, 45168844.0, 7216075.0, 5601663.5, 7595461.5, 21814230.0, 11125838.0, 66492236.0, 10355025.0, 10761022.0, 1731816.125, 16742478.0, 34655052.0, 4836802.5, 7302157.5, 19926340.0, 32434060.0, 20422028.0, 18703928.0, 6791870.0, 84606104.0, 19208380.0, 5963771.0, 4313013.5, 4523350.5, 41435304.0, 20353062.0, 30637276.0, 127997128.0, 24844436.0, 50038188.0, 9427298.0, 6725183.0, 21572226.0, 6154284.5, 8254649.0, 12407094.0, 5023791.0, 15375994.0, 14976095.0, 11220951.0, 6162021.0, 33251548.0, 14972545.0, 21483986.0, 152439792.0, 38424924.0, 11096877.0, 7661095.0, 7018556.0, 46353156.0, 15397861.0, 3013791.75, 57520704.0, 29039984.0, 9280528.0, 12316143.0, 61882900.0, 16517925.0, 104739856.0, 6355545.5, 15902657.0, 8188424.0, 22811426.0, 19126742.0, 26397676.0, 13781229.0, 19875452.0] 
 
Loss from sanitized data: 
[129.86984252929688, 4503400960.0, 0.0, 0.0, 0.0, 1058847.0, 9167.7001953125, 753.635009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000442216987721622, 0.0, 0.0, 0.0, 0.002505896147340536, 0.0, 0.0, 0.00147405662573874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00073702831286937, 0.0] 

 
 Learning Rate: 5e-07 
Number Epochs: 1500 
weight decay: 0
Training Loss: L1Loss()
Test Loss: L1Loss() 
self.self.optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-07
    weight_decay: 0
)
Model Architecture: Autoencoder(
  (encoder): Sequential(
    (0): Linear(in_features=100, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=100, out_features=100, bias=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Linear(in_features=100, out_features=100, bias=True)
    (5): LeakyReLU(negative_slope=0.01)
  )
)
Training completed in: 1177.25 minutes
