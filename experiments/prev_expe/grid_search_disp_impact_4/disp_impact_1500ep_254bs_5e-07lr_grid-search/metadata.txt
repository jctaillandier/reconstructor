Epochs: 1500 
 
Total of 0 / 100 are now closer to original data (L1 distance). 
 
Epoch of lowest loss: 143 
 
Average lowest loss: 75006568.509375143 
 
Lowest lost Generated:
 [13363357.0, 4596230144.0, 3963372.75, 1663435.125, 67630480.0, 119582448.0, 13622748.0, 11782430.0, 5933615.5, 6773895.0, 5937811.5, 13725132.0, 2713900.0, 4381251.5, 11574045.0, 929333.5, 14057996.0, 3677201.0, 1508402.25, 5855564.5, 14350366.0, 6139354.0, 4333282.5, 36919536.0, 23901920.0, 20102630.0, 56769904.0, 20440416.0, 62349944.0, 6721015.0, 27708656.0, 108479936.0, 5213786.0, 34372884.0, 43201328.0, 34920444.0, 6399878.0, 51799696.0, 7493457.5, 4284980.5, 63990624.0, 28993292.0, 11855657.0, 76083624.0, 27945750.0, 17044204.0, 17207360.0, 140266992.0, 19270572.0, 36002468.0, 7239075.0, 97322504.0, 6525036.0, 867098.3125, 3495828.5, 1888396.125, 71936248.0, 2458796.0, 10343839.0, 17811682.0, 62110484.0, 14887319.0, 18618862.0, 12387833.0, 41811776.0, 24499534.0, 64854512.0, 16837844.0, 16162282.0, 4576812.5, 4409843.0, 26122254.0, 50182168.0, 17907062.0, 115775568.0, 14039736.0, 9845144.0, 6785483.0, 3554526.75, 1922632.5, 12174485.0, 60299692.0, 38775060.0, 697392.875, 10774200.0, 32768766.0, 96669800.0, 52596552.0, 54372988.0, 2564092.5, 81963624.0, 58274684.0, 4188363.25, 43518888.0, 31725238.0, 35648112.0, 90194560.0, 34930776.0, 2882911.5, 88985968.0] 
 
Loss from sanitized data: 
[129.86984252929688, 4503400960.0, 0.0, 0.0, 0.0, 1058847.0, 9167.7001953125, 753.635009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000442216987721622, 0.0, 0.0, 0.0, 0.002505896147340536, 0.0, 0.0, 0.00147405662573874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00073702831286937, 0.0] 

 
 Learning Rate: 5e-07 
Number Epochs: 1500 
weight decay: 0
Training Loss: L1Loss()
Test Loss: L1Loss() 
self.self.optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-07
    weight_decay: 0
)
Model Architecture: Autoencoder(
  (encoder): Sequential(
    (0): Linear(in_features=100, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=100, out_features=100, bias=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Linear(in_features=100, out_features=100, bias=True)
    (5): LeakyReLU(negative_slope=0.01)
  )
)
Training completed in: 1539.83 minutes
