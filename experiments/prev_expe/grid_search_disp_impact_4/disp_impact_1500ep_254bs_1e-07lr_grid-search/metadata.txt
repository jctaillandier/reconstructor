Epochs: 1500 
 
Total of 0 / 100 are now closer to original data (L1 distance). 
 
Epoch of lowest loss: 842 
 
Average lowest loss: 53092258.584375842 
 
Lowest lost Generated:
 [1898789.875, 4548735488.0, 2652314.75, 2488071.0, 4214905.5, 109905224.0, 5425578.5, 4692669.5, 2121132.5, 1683268.875, 3917366.0, 8155470.0, 7099857.0, 19821136.0, 4875872.5, 6453303.0, 677932.75, 4729577.5, 1980549.375, 10938283.0, 4150248.75, 9614701.0, 7014722.5, 2854123.25, 10569378.0, 5897455.5, 4756457.0, 3970595.25, 4526118.5, 2055824.5, 10998817.0, 2270140.0, 3839729.5, 2959367.0, 10204859.0, 2295780.75, 11448623.0, 961304.0625, 9365942.0, 8701274.0, 2730382.5, 24441822.0, 2826896.25, 2767854.5, 10564490.0, 2833121.0, 6952302.5, 10942460.0, 11053919.0, 1001759.875, 4488220.5, 4908202.0, 6134165.5, 341542.0625, 14416939.0, 1618432.75, 14783152.0, 3488929.0, 2419432.5, 3924395.5, 2867884.0, 9686850.0, 2741194.5, 7465413.0, 10769694.0, 3493002.5, 3151957.5, 12124242.0, 5516304.5, 7688952.0, 7717269.0, 1554617.75, 4660450.0, 6858953.5, 2555807.75, 2870403.25, 2081419.375, 10992070.0, 4596162.0, 10505195.0, 2583043.5, 7262598.0, 7013163.0, 1644501.875, 4941816.0, 4629253.0, 5687599.0, 2277824.25, 810538.3125, 2506570.75, 12225749.0, 9936053.0, 7381074.0, 20255240.0, 7782708.5, 12393925.0, 22425682.0, 17838390.0, 23074568.0, 13097051.0] 
 
Loss from sanitized data: 
[129.86984252929688, 4503400960.0, 0.0, 0.0, 0.0, 1058847.0, 9167.7001953125, 753.635009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000442216987721622, 0.0, 0.0, 0.0, 0.002505896147340536, 0.0, 0.0, 0.00147405662573874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00073702831286937, 0.0] 

 
 Learning Rate: 1e-07 
Number Epochs: 1500 
weight decay: 0
Training Loss: L1Loss()
Test Loss: L1Loss() 
self.self.optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-07
    weight_decay: 0
)
Model Architecture: Autoencoder(
  (encoder): Sequential(
    (0): Linear(in_features=100, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=100, out_features=100, bias=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Linear(in_features=100, out_features=100, bias=True)
    (5): LeakyReLU(negative_slope=0.01)
  )
)
Training completed in: 1539.88 minutes
