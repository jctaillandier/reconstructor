Epochs: 1500 
 
Total of 0 / 100 are now closer to original data (L1 distance). 
 
Epoch of lowest loss: 1114 
 
Average lowest loss: 50353329.193906251114 
 
Lowest lost Generated:
 [185966.65625, 4553974272.0, 995044.6875, 2620679.75, 945278.625, 107186224.0, 3117173.5, 941920.0625, 1345148.125, 926000.375, 651650.5625, 170009.09375, 3086908.75, 234425.28125, 294501.25, 380819.5625, 1062445.0, 435773.0625, 139705.4375, 668015.75, 498051.3125, 2092353.25, 385529.75, 254676.9375, 114326.140625, 438921.5625, 1846025.25, 361151.40625, 532051.875, 2331638.75, 4837430.0, 1289055.375, 5843749.5, 3582817.25, 3329914.25, 4037757.5, 304550.75, 28987814.0, 3089611.25, 5940488.0, 1095592.5, 324156.34375, 194953.609375, 1770813.0, 799087.5, 3461294.0, 436540.9375, 5947607.0, 3812441.0, 576133.1875, 8660485.0, 787310.125, 648253.0625, 4584070.0, 342944.125, 150654.390625, 240828.53125, 5479701.5, 5995883.5, 447886.78125, 2623051.75, 2928383.75, 2681037.5, 4037461.25, 1422653.0, 722153.75, 768014.125, 4566989.5, 8746712.0, 151476.015625, 12840679.0, 602212.0625, 38249876.0, 9189354.0, 1143551.0, 3803647.5, 1338627.875, 12430020.0, 1557617.0, 2336024.75, 6110713.5, 4645186.5, 32010634.0, 251242.390625, 2673911.75, 3159704.25, 3256415.75, 638709.3125, 1920761.625, 663766.5, 2475942.25, 1692093.375, 11829568.0, 24055370.0, 684272.625, 7320046.5, 8448622.0, 3919741.5, 2691322.0, 9528842.0] 
 
Loss from sanitized data: 
[129.86984252929688, 4503400960.0, 0.0, 0.0, 0.0, 1058847.0, 9167.7001953125, 753.635009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000442216987721622, 0.0, 0.0, 0.0, 0.002505896147340536, 0.0, 0.0, 0.00147405662573874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00073702831286937, 0.0] 

 
 Learning Rate: 1e-07 
Number Epochs: 1500 
weight decay: 0
Training Loss: L1Loss()
Test Loss: L1Loss() 
self.self.optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-07
    weight_decay: 0
)
Model Architecture: Autoencoder(
  (encoder): Sequential(
    (0): Linear(in_features=100, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=100, out_features=100, bias=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Linear(in_features=100, out_features=100, bias=True)
    (5): LeakyReLU(negative_slope=0.01)
  )
)
Training completed in: 1177.58 minutes
