Epochs: 500 
 
Total of 0 / 100 are now closer to original data (L1 distance). 
 
Epoch of lowest loss: 124 
 
Lowest lost Generated:
 [14701604.0, 4705336832.0, 50648676.0, 13645946.0, 25684818.0, 162876320.0, 13121310.0, 58935604.0, 39439592.0, 81595936.0, 29972562.0, 14072875.0, 12358190.0, 9346867.0, 41498120.0, 48449932.0, 15287297.0, 21156252.0, 47025764.0, 115316512.0, 72931560.0, 21001142.0, 37416096.0, 8585720.0, 7735479.5, 5938107.0, 39177552.0, 15415829.0, 214715.15625, 54139212.0, 18785736.0, 5160560.5, 13064171.0, 17671666.0, 9588528.0, 6313356.5, 20591444.0, 118042528.0, 27200020.0, 8161456.5, 47416756.0, 53784696.0, 24365234.0, 13400091.0, 3634827.75, 12122436.0, 32548994.0, 14917983.0, 8770026.0, 28134874.0, 561175.9375, 26274340.0, 90273264.0, 8040093.0, 1789319.0, 5161386.5, 25925366.0, 50097412.0, 8513535.0, 15987204.0, 7981263.5, 168775456.0, 57330940.0, 8042641.0, 9545957.0, 6158760.5, 9952610.0, 10624599.0, 29791794.0, 33629384.0, 9332571.0, 23483836.0, 7648772.0, 18650970.0, 80678512.0, 10497212.0, 12838815.0, 55369300.0, 28506306.0, 21957802.0, 112957728.0, 108166392.0, 11225474.0, 73557432.0, 60426916.0, 9864193.0, 9728482.0, 51771084.0, 1971386.0, 12024694.0, 1214414.625, 16533064.0, 42255864.0, 130389264.0, 43641436.0, 11600418.0, 21031560.0, 60204128.0, 18320736.0, 42342728.0] 
 
Loss from sanitized data: 
[129.86984252929688, 4503400960.0, 0.0, 0.0, 0.0, 1058847.0, 9167.7001953125, 753.635009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000442216987721622, 0.0, 0.0, 0.0, 0.002505896147340536, 0.0, 0.0, 0.00147405662573874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00073702831286937, 0.0] 

 
 Learning Rate: 1e-06 
Number Epochs: 500 
weight decay: 0
Training Loss: L1Loss()
Test Loss: L1Loss() 
self.self.optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-06
    weight_decay: 0
)
Model Architecture: Autoencoder(
  (encoder): Sequential(
    (0): Linear(in_features=100, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=100, out_features=100, bias=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Linear(in_features=100, out_features=100, bias=True)
    (5): LeakyReLU(negative_slope=0.01)
  )
)
Training completed in: 262.02 minutes
