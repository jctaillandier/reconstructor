Epochs: 500 
 
Total of 0 / 100 are now closer to original data (L1 distance). 
 
Epoch of lowest loss: 342 
 
Lowest lost Generated:
 [4959996.5, 4591368704.0, 1439470.0, 19654882.0, 1426028.625, 125737984.0, 4446709.5, 11703510.0, 6534997.0, 4016707.25, 3110996.5, 4713067.0, 1875760.25, 10989633.0, 12606494.0, 11195252.0, 5143681.0, 13062040.0, 63002708.0, 29784858.0, 21521924.0, 6614877.0, 12820563.0, 669257.875, 7734722.5, 12297601.0, 14963836.0, 10896571.0, 4913131.5, 14320418.0, 10117515.0, 6099223.5, 11127261.0, 10862457.0, 13175756.0, 2850754.75, 1946688.25, 11216983.0, 27643548.0, 7439498.5, 8419272.0, 11080638.0, 25294024.0, 91617528.0, 10358399.0, 90356592.0, 17868808.0, 15782920.0, 44514520.0, 3712974.0, 4345237.0, 2243632.25, 31882296.0, 15302196.0, 13862568.0, 83827200.0, 8237888.5, 44516204.0, 55947584.0, 46857152.0, 66147500.0, 62075976.0, 83564200.0, 7695430.5, 19325462.0, 4127641.25, 16684568.0, 70881048.0, 12586736.0, 12197463.0, 18902780.0, 4985300.5, 26580712.0, 55953540.0, 30896916.0, 75350128.0, 20010638.0, 16897394.0, 2964471.75, 30673336.0, 54228168.0, 36665824.0, 12405545.0, 18360540.0, 80780624.0, 5422964.0, 43685312.0, 35243896.0, 36435596.0, 13928334.0, 68371384.0, 26205158.0, 8486808.0, 13595652.0, 28611702.0, 87865352.0, 12836339.0, 48282692.0, 32126006.0, 45788104.0] 
 
Loss from sanitized data: 
[129.86984252929688, 4503400960.0, 0.0, 0.0, 0.0, 1058847.0, 9167.7001953125, 753.635009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000442216987721622, 0.0, 0.0, 0.0, 0.002505896147340536, 0.0, 0.0, 0.00147405662573874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00073702831286937, 0.0] 

 
 Learning Rate: 1e-06 
Number Epochs: 500 
weight decay: 0
Training Loss: L1Loss()
Test Loss: L1Loss() 
self.self.optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-06
    weight_decay: 0
)
Model Architecture: Autoencoder(
  (encoder): Sequential(
    (0): Linear(in_features=100, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=100, out_features=100, bias=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Linear(in_features=100, out_features=100, bias=True)
    (5): LeakyReLU(negative_slope=0.01)
  )
)
Training completed in: 354.62 minutes
