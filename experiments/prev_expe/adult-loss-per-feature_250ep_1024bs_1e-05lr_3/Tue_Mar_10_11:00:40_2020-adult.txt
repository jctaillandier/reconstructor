Epochs: 250 
Lowest lost Generated: [0.04609660804271698, 0.06167558953166008, 0.05638918653130531, 0.007062309887260199, 0.006855922285467386, 0.04428032785654068, 0.033550068736076355, 0.02348063513636589, 0.02805556356906891, 0.03816420957446098, 0.01959393545985222, 0.03583637624979019, 0.04306502267718315, 0.049361832439899445, 0.04064906761050224, 0.02688625082373619, 0.003244588151574135, 0.009287644177675247, 0.03896510228514671, 0.012417496182024479, 0.02845383621752262, 0.004729981068521738, 0.04324857145547867, 0.04892228916287422, 0.023964136838912964, 0.030417323112487793, 0.022593969479203224, 0.036448560655117035, 0.04600650072097778, 0.05085347965359688, 0.048328522592782974, 0.01683763414621353, 0.033117420971393585, 0.012131700292229652, 0.027556153014302254, 0.1413358598947525, 0.01591566763818264, 0.05182874947786331, 0.03956461325287819, 0.035435765981674194, 0.044185820966959, 0.03213544189929962, 0.04516090080142021, 0.028018765151500702, 0.024187425151467323, 0.047361280769109726, 0.017479492351412773, 0.019364655017852783, 0.019113333895802498, 0.04765181243419647, 0.005993172060698271, 0.014813143759965897, 0.0245104618370533, 0.027242228388786316, 0.03394651785492897] 
Loss from sanitized data: [0.05256110057234764, 0.0864541232585907, 0.07383452355861664, 0.007562745362520218, 0.008614004589617252, 0.042730867862701416, 0.03511001914739609, 0.022079024463891983, 0.029735738411545753, 0.03760433569550514, 0.01658485271036625, 0.02886626124382019, 0.11282595992088318, 0.05046327784657478, 0.032769814133644104, 0.03044651634991169, 0.0037966142408549786, 0.009806890971958637, 0.02738809771835804, 0.013939634896814823, 0.029543401673436165, 0.004035644698888063, 0.045904915779829025, 0.03026065230369568, 0.04362742602825165, 0.05017658695578575, 0.04333125799894333, 0.04311924800276756, 0.04636986926198006, 0.049169041216373444, 0.04927089437842369, 0.01522365678101778, 0.046623654663562775, 0.011754042468965054, 0.036036740988492966, 0.1420794278383255, 0.014897923916578293, 0.034529928117990494, 0.029869087040424347, 0.027746254578232765, 0.04188912361860275, 0.04309840872883797, 0.04642263799905777, 0.05620449781417847, 0.0363001748919487, 0.025420289486646652, 0.017737356945872307, 0.017655298113822937, 0.020437531173229218, 0.06424584239721298, 0.005110893398523331, 0.014975331723690033, 0.02715211920440197, 0.026049572974443436, 0.0696210265159607] 
Is L1 better?
  
        age     fnlwgt education-num capital-gain capital-loss hours-per-week       sex=0       sex=1    income=0     income=1 workclass=? workclass=Local-gov workclass=Private workclass=Self-emp-inc workclass=Self-emp-not-inc workclass=State-gov workclass=Without-pay education=10th education=11th education=7th-8th education=Assoc-acdm education=Assoc-voc education=Bachelors education=HS-grad education=Masters education=Some-college marital-status=Divorced marital-status=Married-civ-spouse marital-status=Never-married marital-status=Separated marital-status=Widowed occupation=? occupation=Adm-clerical occupation=Armed-Forces occupation=Craft-repair occupation=Exec-managerial occupation=Handlers-cleaners occupation=Other-service occupation=Prof-specialty occupation=Sales relationship=Husband relationship=Not-in-family relationship=Other-relative relationship=Own-child relationship=Unmarried relationship=Wife race=Amer-Indian-Eskimo race=Asian-Pac-Islander race=Black race=White native-country=? native-country=Japan native-country=Mexico native-country=Nicaragua native-country=United-States
       True       True          True         True         True          False        True       False        True        False       False               False              True                   True                      False                True                  True           True          False              True                 True               False                True             False              True                   True                    True                              True                         True                    False                   True        False                    True                   False                    True                       True                        False                    False                     False            False                False                       True                        True                   True                   True             False                    True                   False       True       True            False                 True                  True                    False                         True
 0.00646449  0.0247785     0.0174453  0.000500435   0.00175808    -0.00154946  0.00155995 -0.00140161  0.00168017 -0.000559874 -0.00300908         -0.00697012         0.0697609             0.00110145                -0.00787925          0.00356027           0.000552026    0.000519247      -0.011577        0.00152214           0.00108957        -0.000694336          0.00265634        -0.0186616         0.0196633              0.0197593               0.0207373                        0.00667069                  0.000363369              -0.00168444            0.000942372  -0.00161398               0.0135062            -0.000377658              0.00848059                0.000743568                  -0.00101774               -0.0172988               -0.00969553      -0.00768951           -0.0022967                   0.010963                  0.00126174              0.0281857              0.0121127         -0.021941             0.000257865             -0.00170936  0.0013242   0.016594     -0.000882279          0.000162188            0.00264166              -0.00119266                    0.0356745
 
 Learning Rate: 1e-05 
Number Epochs: 250 
weight decay: 0
Training Loss: L1Loss()
Test Loss: L1Loss() 
self.self.optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)
Model Architecture: Autoencoder(
  (encoder): Sequential(
    (0): Linear(in_features=55, out_features=55, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=55, out_features=55, bias=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Linear(in_features=55, out_features=55, bias=True)
    (5): LeakyReLU(negative_slope=0.01)
  )
)
Training completed in: 20.67 minutes
Train loss values: [185.7889404296875, 135.52098083496094, 117.77049255371094, 107.2870101928711, 99.58486938476562, 94.66191101074219, 89.11280059814453, 84.2061767578125, 81.34081268310547, 79.3975601196289, 77.89633178710938, 76.39586639404297, 74.44686889648438, 72.47423553466797, 70.68534851074219, 69.45650482177734, 67.22612762451172, 65.87287139892578, 64.8244857788086, 64.81704711914062, 64.02832794189453, 63.64902877807617, 62.82625961303711, 61.67768478393555, 60.90203857421875, 59.871070861816406, 58.72425842285156, 57.808326721191406, 57.00324630737305, 56.417152404785156, 55.83748245239258, 55.320552825927734, 54.92581558227539, 54.78151321411133, 54.25032043457031, 53.548179626464844, 53.439178466796875, 52.62141799926758, 51.95681381225586, 51.53779602050781, 51.53586959838867, 51.946937561035156, 52.12092590332031, 51.48090744018555, 51.92313003540039, 51.62097930908203, 51.14609909057617, 50.15784454345703, 50.208953857421875, 50.65836715698242, 49.44133377075195, 49.04221725463867, 48.760009765625, 47.96783447265625, 48.05954360961914, 48.10330581665039, 48.95801544189453, 48.73942947387695, 47.85358810424805, 47.70672607421875, 47.274383544921875, 47.8671875, 48.174766540527344, 48.26683807373047, 48.94855880737305, 48.89543533325195, 48.32780838012695, 47.49646759033203, 47.472625732421875, 46.992767333984375, 46.55891418457031, 46.81678771972656, 47.468326568603516, 47.840946197509766, 48.102291107177734, 47.567100524902344, 47.10913848876953, 46.733360290527344, 46.26030349731445, 46.499473571777344, 46.56657409667969, 46.48033142089844, 46.62001037597656, 46.53377151489258, 45.641178131103516, 45.713199615478516, 44.58574295043945, 44.895179748535156, 45.639617919921875, 46.16745376586914, 46.418128967285156, 46.546573638916016, 47.49760437011719, 46.62408447265625, 46.70607376098633, 45.2742805480957, 45.722476959228516, 45.178192138671875, 44.225379943847656, 44.09372329711914, 44.010276794433594, 43.819881439208984, 43.34052276611328, 43.177223205566406, 42.51199722290039, 41.93291473388672, 42.32640838623047, 41.51667785644531, 42.297813415527344, 41.444725036621094, 41.38639831542969, 40.45120620727539, 39.9046516418457, 40.02599334716797, 39.77592849731445, 40.01443862915039, 39.640384674072266, 39.93681335449219, 39.787471771240234, 39.901161193847656, 40.268856048583984, 39.885440826416016, 39.28970718383789, 39.19758224487305, 39.98657989501953, 39.71982955932617, 39.77454376220703, 38.95960235595703, 39.778526306152344, 40.43717575073242, 39.60639572143555, 39.268402099609375, 38.480224609375, 38.71755599975586, 40.0107307434082, 39.419837951660156, 40.327728271484375, 40.99913024902344, 40.88249206542969, 39.195220947265625, 39.921932220458984, 40.447174072265625, 39.702613830566406, 39.36320114135742, 38.686988830566406, 38.47575759887695, 40.36772155761719, 40.04690170288086, 39.28524398803711, 37.87690353393555, 37.05754470825195, 37.28355026245117, 36.535179138183594, 35.900634765625, 36.896610260009766, 38.74337387084961, 39.67237091064453, 39.20054626464844, 37.59136962890625, 37.84065628051758, 35.82501220703125, 36.1069221496582, 36.73240280151367, 37.38031005859375, 39.96644973754883, 40.107505798339844, 40.44778060913086, 40.643123626708984, 39.33343505859375, 38.90107345581055, 38.89906692504883, 38.59854507446289, 39.556209564208984, 38.89824295043945, 39.72301483154297, 38.50545120239258, 38.97667694091797, 39.68589401245117, 40.00154495239258, 42.34090042114258, 42.5715217590332, 42.671634674072266, 40.857784271240234, 39.597023010253906, 37.741878509521484, 37.134525299072266, 36.42562484741211, 37.38618087768555, 40.12930679321289, 40.17803955078125, 39.33070755004883, 37.98982620239258, 37.32445526123047, 36.319786071777344, 36.59047317504883, 37.64445877075195, 39.2950325012207, 40.891845703125, 39.447792053222656, 38.48774337768555, 37.176055908203125, 36.79754638671875, 35.377891540527344, 35.01914596557617, 34.22876739501953, 34.03174591064453, 35.069923400878906, 34.9033317565918, 34.903621673583984, 34.0675163269043, 32.85710525512695, 32.69105911254883, 32.38994598388672, 33.371124267578125, 34.454986572265625, 35.117042541503906, 36.408447265625, 35.66123580932617, 35.951412200927734, 34.4773063659668, 33.47767639160156, 33.49609375, 34.65327453613281, 34.5117301940918, 34.12628173828125, 33.924537658691406, 32.57884979248047, 32.35272979736328, 31.957746505737305, 31.7515869140625, 32.31446075439453, 34.385284423828125, 34.9318733215332, 34.69622802734375, 35.09354782104492, 35.591190338134766, 34.802215576171875, 35.18722152709961, 34.05048370361328, 34.08030319213867, 34.630558013916016, 33.49251174926758, 33.361297607421875, 33.477020263671875, 33.720008850097656, 33.44129180908203, 33.77591323852539, 33.776527404785156, 33.46661376953125, 32.98847198486328] 
Test loss values: [tensor(0.1360), tensor(0.1222), tensor(0.1054), tensor(0.0988), tensor(0.0924), tensor(0.0877), tensor(0.0828), tensor(0.0781), tensor(0.0771), tensor(0.0759), tensor(0.0739), tensor(0.0727), tensor(0.0704), tensor(0.0669), tensor(0.0668), tensor(0.0648), tensor(0.0634), tensor(0.0618), tensor(0.0621), tensor(0.0612), tensor(0.0609), tensor(0.0606), tensor(0.0590), tensor(0.0581), tensor(0.0580), tensor(0.0565), tensor(0.0551), tensor(0.0550), tensor(0.0537), tensor(0.0532), tensor(0.0524), tensor(0.0523), tensor(0.0516), tensor(0.0513), tensor(0.0497), tensor(0.0510), tensor(0.0502), tensor(0.0492), tensor(0.0486), tensor(0.0496), tensor(0.0507), tensor(0.0482), tensor(0.0501), tensor(0.0465), tensor(0.0496), tensor(0.0487), tensor(0.0462), tensor(0.0471), tensor(0.0478), tensor(0.0465), tensor(0.0466), tensor(0.0466), tensor(0.0451), tensor(0.0437), tensor(0.0453), tensor(0.0471), tensor(0.0466), tensor(0.0462), tensor(0.0457), tensor(0.0460), tensor(0.0461), tensor(0.0470), tensor(0.0458), tensor(0.0468), tensor(0.0466), tensor(0.0478), tensor(0.0472), tensor(0.0452), tensor(0.0428), tensor(0.0445), tensor(0.0445), tensor(0.0454), tensor(0.0459), tensor(0.0454), tensor(0.0434), tensor(0.0436), tensor(0.0455), tensor(0.0425), tensor(0.0418), tensor(0.0428), tensor(0.0432), tensor(0.0436), tensor(0.0440), tensor(0.0426), tensor(0.0419), tensor(0.0407), tensor(0.0426), tensor(0.0422), tensor(0.0438), tensor(0.0433), tensor(0.0434), tensor(0.0442), tensor(0.0420), tensor(0.0431), tensor(0.0446), tensor(0.0441), tensor(0.0433), tensor(0.0423), tensor(0.0429), tensor(0.0414), tensor(0.0422), tensor(0.0408), tensor(0.0420), tensor(0.0405), tensor(0.0385), tensor(0.0392), tensor(0.0381), tensor(0.0428), tensor(0.0411), tensor(0.0389), tensor(0.0399), tensor(0.0368), tensor(0.0385), tensor(0.0370), tensor(0.0389), tensor(0.0378), tensor(0.0388), tensor(0.0392), tensor(0.0398), tensor(0.0393), tensor(0.0366), tensor(0.0371), tensor(0.0375), tensor(0.0363), tensor(0.0389), tensor(0.0350), tensor(0.0370), tensor(0.0356), tensor(0.0372), tensor(0.0379), tensor(0.0382), tensor(0.0365), tensor(0.0358), tensor(0.0383), tensor(0.0385), tensor(0.0410), tensor(0.0408), tensor(0.0393), tensor(0.0357), tensor(0.0388), tensor(0.0412), tensor(0.0391), tensor(0.0376), tensor(0.0371), tensor(0.0387), tensor(0.0391), tensor(0.0391), tensor(0.0380), tensor(0.0377), tensor(0.0359), tensor(0.0352), tensor(0.0349), tensor(0.0338), tensor(0.0339), tensor(0.0376), tensor(0.0363), tensor(0.0365), tensor(0.0363), tensor(0.0373), tensor(0.0358), tensor(0.0339), tensor(0.0333), tensor(0.0339), tensor(0.0395), tensor(0.0379), tensor(0.0366), tensor(0.0386), tensor(0.0383), tensor(0.0366), tensor(0.0398), tensor(0.0369), tensor(0.0396), tensor(0.0369), tensor(0.0362), tensor(0.0380), tensor(0.0359), tensor(0.0395), tensor(0.0411), tensor(0.0423), tensor(0.0451), tensor(0.0415), tensor(0.0407), tensor(0.0374), tensor(0.0369), tensor(0.0364), tensor(0.0350), tensor(0.0353), tensor(0.0383), tensor(0.0356), tensor(0.0375), tensor(0.0398), tensor(0.0368), tensor(0.0351), tensor(0.0346), tensor(0.0339), tensor(0.0408), tensor(0.0385), tensor(0.0380), tensor(0.0401), tensor(0.0367), tensor(0.0351), tensor(0.0353), tensor(0.0342), tensor(0.0337), tensor(0.0335), tensor(0.0347), tensor(0.0367), tensor(0.0353), tensor(0.0339), tensor(0.0334), tensor(0.0341), tensor(0.0334), tensor(0.0348), tensor(0.0382), tensor(0.0367), tensor(0.0405), tensor(0.0378), tensor(0.0399), tensor(0.0403), tensor(0.0393), tensor(0.0395), tensor(0.0397), tensor(0.0398), tensor(0.0376), tensor(0.0379), tensor(0.0365), tensor(0.0356), tensor(0.0357), tensor(0.0343), tensor(0.0351), tensor(0.0349), tensor(0.0405), tensor(0.0389), tensor(0.0405), tensor(0.0347), tensor(0.0381), tensor(0.0334), tensor(0.0359), tensor(0.0333), tensor(0.0372), tensor(0.0338), tensor(0.0344), tensor(0.0345), tensor(0.0332), tensor(0.0337), tensor(0.0341), tensor(0.0330), tensor(0.0349), tensor(0.0342), tensor(0.0369)]
