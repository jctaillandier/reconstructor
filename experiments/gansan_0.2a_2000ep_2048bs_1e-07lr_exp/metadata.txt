Epochs: 2000 
 
Total of 61 / 88 are now closer to original data (L1 distance). 
 
Epoch of lowest loss: 1982 
 
Average lowest loss: 11.2698554252340491982 
 
Lowest lost Generated:
 [0.18788017332553864, 0.23163777589797974, 0.13906057178974152, 916.3347778320312, 68.63471984863281, 0.10974106937646866, 0.32562336325645447, 0.3748498260974884, 0.25864577293395996, 0.24227216839790344, 0.03128957003355026, 0.07472389936447144, 0.3152082562446594, 0.037496935576200485, 0.08549019694328308, 0.04552346467971802, 0.0010957226622849703, 0.02824818156659603, 0.036394745111465454, 0.012761488556861877, 0.0075873034074902534, 0.009290503337979317, 0.018544016405940056, 0.013130870647728443, 0.03309234231710434, 0.047662403434515, 0.17705218493938446, 0.012785135768353939, 0.3304453194141388, 0.053930968046188354, 0.004003991838544607, 0.018320895731449127, 0.21932353079319, 0.4697662591934204, 0.31793883442878723, 0.12328486889600754, 0.00058548100059852, 0.13472715020179749, 0.12861129641532898, 0.03011658415198326, 0.06409865617752075, 0.11093086004257202, 0.008012838661670685, 0.13762594759464264, 0.021190525963902473, 0.12354495376348495, 0.029952257871627808, 0.05162540823221207, 0.41482895612716675, 0.25439462065696716, 0.028598688542842865, 0.15592598915100098, 0.013567153364419937, 0.030630310997366905, 0.09261725097894669, 0.0149275166913867, 0.1577102392911911, 0.0007953824242576957, 0.004280189052224159, 0.0058065662160515785, 0.0020324792712926865, 0.003481844440102577, 0.006088548805564642, 0.00183412479236722, 0.003945390228182077, 0.005796968936920166, 0.0016115225153043866, 0.0038502460811287165, 0.003292381763458252, 0.0022323604207485914, 0.0009345684084109962, 0.0012364843860268593, 0.002669102745130658, 0.004652299452573061, 0.002457746071740985, 0.002924778265878558, 0.006758164614439011, 0.018637172877788544, 0.005738178733736277, 0.001547818654216826, 0.01377055887132883, 0.004074465949088335, 0.0046606543473899364, 0.0029827908147126436, 0.0012733781477436423, 0.00479706609621644, 0.25233331322669983, 0.002957897027954459] 
 
Loss from sanitized data: 
[0.22626563906669617, 0.23936907947063446, 0.16782443225383759, 916.3363037109375, 68.6352767944336, 0.13819925487041473, 0.4609375, 0.4609375, 0.377505898475647, 0.377505898475647, 0.14652122557163239, 0.07900943607091904, 0.39077240228652954, 0.037441037595272064, 0.15978772938251495, 0.07502947747707367, 0.007075471803545952, 0.03080778382718563, 0.03581957519054413, 0.03508254885673523, 0.004716981202363968, 0.11379717290401459, 0.01547759398818016, 0.012824292294681072, 0.03478773683309555, 0.11364976316690445, 0.2573702931404114, 0.012824292294681072, 0.45548349618911743, 0.056456368416547775, 0.030070755630731583, 0.015182782895863056, 0.3708726465702057, 0.5125294923782349, 0.4714033007621765, 0.2682783007621765, 0.22685730457305908, 0.18145637214183807, 0.2619398534297943, 0.15831367671489716, 0.06765919923782349, 0.11143867671489716, 0.00589622650295496, 0.22051887214183807, 0.03596698120236397, 0.16686320304870605, 0.03066037781536579, 0.052181605249643326, 0.5246167182922363, 0.3652712404727936, 0.02800707519054413, 0.2824292480945587, 0.010613207705318928, 0.057045988738536835, 0.16362027823925018, 0.008402122184634209, 0.22258254885673523, 0.000589622650295496, 0.007075471803545952, 0.002211084822192788, 0.002211084822192788, 0.002800707472488284, 0.00294811325147748, 0.002653301926329732, 0.007665094453841448, 0.002800707472488284, 0.00397995300590992, 0.00486438674852252, 0.001179245300590992, 0.001916273613460362, 0.002358490601181984, 0.001916273613460362, 0.002653301926329732, 0.002653301926329732, 0.007222877349704504, 0.002063679276034236, 0.002358490601181984, 0.032871462404727936, 0.000884433975443244, 0.006633254699409008, 0.015182782895863056, 0.00294811325147748, 0.01179245300590992, 0.000294811325147748, 0.002800707472488284, 0.001031839638017118, 0.1391509473323822, 0.000884433975443244] 

 
 Learning Rate: 1e-07 
Number Epochs: 2000 
weight decay: 0
Training Loss: L1Loss()
Test Loss: L1Loss() 
self.self.optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-07
    weight_decay: 0
)
Model Architecture: Autoencoder(
  (encoder): Sequential(
    (0): Linear(in_features=88, out_features=88, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=88, out_features=88, bias=True)
    (3): LeakyReLU(negative_slope=0.01)
  )
)
Training completed in: 246.20 minutes
