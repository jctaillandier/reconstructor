Epochs: 5 
 
Total of 0 / 100 are now closer to original data (L1 distance). 
 
Epoch of lowest loss: 4 
 
Average lowest loss: 853121600.564 
 
Lowest lost Generated:
 [481840096.0, 6507887616.0, 161051168.0, 313910208.0, 225785680.0, 170998240.0, 22912230.0, 601023360.0, 279832000.0, 28887446.0, 911136256.0, 883522432.0, 400189632.0, 446544864.0, 602418112.0, 606500800.0, 672742208.0, 185748144.0, 668399040.0, 298678688.0, 687860864.0, 991805568.0, 217841920.0, 408088320.0, 717361024.0, 1011405248.0, 434093312.0, 174747248.0, 388301760.0, 1110599680.0, 638366912.0, 525843616.0, 222383120.0, 152959968.0, 644654400.0, 594966784.0, 881123328.0, 303034208.0, 646592064.0, 1185947392.0, 273255872.0, 276901344.0, 1028453888.0, 399013728.0, 815726464.0, 408175488.0, 922353792.0, 621188032.0, 471925056.0, 63441076.0, 183950368.0, 316137728.0, 695874624.0, 1021702464.0, 826048896.0, 843619648.0, 1464392704.0, 1243477888.0, 487579808.0, 998617792.0, 123537864.0, 994803968.0, 921805504.0, 1389857408.0, 1512642944.0, 867370496.0, 1351334528.0, 1445177344.0, 1700267776.0, 1458724096.0, 911417408.0, 1536433664.0, 596277376.0, 1437572608.0, 1265051264.0, 1196261632.0, 1244921600.0, 1418733312.0, 233427840.0, 1694750976.0, 1163624192.0, 1332974976.0, 1272235520.0, 996383232.0, 1044605760.0, 860870016.0, 889608448.0, 1001524928.0, 706554496.0, 1359065472.0, 1601916544.0, 1528605824.0, 1368849664.0, 1425418496.0, 427310912.0, 976771392.0, 998278848.0, 853267584.0, 1276167552.0, 1157934976.0] 
 
Loss from sanitized data: 
[129.86984252929688, 4503400960.0, 0.0, 0.0, 0.0, 1058847.0, 9167.7001953125, 753.635009765625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000442216987721622, 0.0, 0.0, 0.0, 0.002505896147340536, 0.0, 0.0, 0.00147405662573874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00073702831286937, 0.0] 

 
 Learning Rate: 1e-05 
Number Epochs: 5 
weight decay: 0
Training Loss: L1Loss()
Test Loss: L1Loss() 
self.self.optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)
Model Architecture: Autoencoder(
  (encoder): Sequential(
    (0): Linear(in_features=100, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=100, out_features=100, bias=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Linear(in_features=100, out_features=100, bias=True)
    (5): LeakyReLU(negative_slope=0.01)
  )
)
Training completed in: 0.98 minutes
