Epochs: 1000 
 
Total of 0 / 82 are now closer to original data (L1 distance). 
 
Epoch of lowest loss: 999 
 
Average lowest loss: 0.08328920201023632999 
 
Lowest lost Generated:
 [0.1677825003862381, 0.10441422462463379, 0.14236478507518768, 0.155084490776062, 0.18104542791843414, 0.14559030532836914, 0.3326413333415985, 0.3259563744068146, 0.40484216809272766, 0.24491478502750397, 0.03177574649453163, 0.07216715067625046, 0.27818742394447327, 0.037105705589056015, 0.08596932888031006, 0.04572056978940964, 0.03692540153861046, 0.04343825951218605, 0.019856054335832596, 0.013839242979884148, 0.021165592595934868, 0.041318733245134354, 0.05535415560007095, 0.176618754863739, 0.30067315697669983, 0.06345997750759125, 0.0023819648195058107, 0.024776048958301544, 0.2269536554813385, 0.14804202318191528, 0.09074297547340393, 0.020470088347792625, 0.3032415211200714, 0.03329365327954292, 0.027474453672766685, 0.12146099656820297, 0.0019425763748586178, 0.13451135158538818, 0.13628526031970978, 0.038354162126779556, 0.05471062660217285, 0.07083368301391602, 0.1112278625369072, 0.006153905764222145, 0.14611290395259857, 0.12952440977096558, 0.037012048065662384, 0.1386891007423401, 0.258147656917572, 0.03399616479873657, 0.15117603540420532, 0.10796802490949631, 0.04683905839920044, 0.019757671281695366, 0.031188765540719032, 0.0918518453836441, 0.008708740584552288, 0.22410821914672852, 0.0012358959065750241, 0.005573587492108345, 0.011581202037632465, 0.005837372969835997, 0.0013076623436063528, 0.012339488603174686, 0.003961092326790094, 0.0016576461493968964, 0.0015090908855199814, 0.0005358572816476226, 0.009500190615653992, 0.0018434192752465606, 0.009414312429726124, 0.012083606794476509, 0.022045237943530083, 0.0010973814642056823, 0.009296134114265442, 0.0013894295552745461, 0.0053223357535898685, 0.01000652089715004, 0.009200972504913807, 0.0014187315246090293, 0.010350463911890984, 0.17505782842636108] 
 
Loss from sanitized data: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] 

 
 Learning Rate: 1e-07 
Number Epochs: 1000 
weight decay: 0
Training Loss: L1Loss()
Test Loss: L1Loss() 
self.self.optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-07
    weight_decay: 0
)
Model Architecture: Autoencoder(
  (encoder): Sequential(
    (0): Linear(in_features=80, out_features=80, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Linear(in_features=80, out_features=82, bias=True)
    (3): LeakyReLU(negative_slope=0.01)
    (4): Linear(in_features=82, out_features=82, bias=True)
    (5): LeakyReLU(negative_slope=0.01)
  )
)
Training completed in: 45.60 minutes
